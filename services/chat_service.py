import os
import requests
from exceptions.custom_exceptions import BadRequestException, PaymentRequiredException, UnauthorizedException, \
    RateLimitException
from services.validation_service import ValidationService


class ChatService:
    OPENAI_CHAT_COMPLETIONS = "https://api.openai.com/v1/chat/completions"
    GROQ_CHAT_COMPLETIONS = "https://api.groq.com/openai/v1/chat/completions"

    @staticmethod
    def ask(message, api_key=None, hf_token=None, use_mock=False, model=None, hf_model=None):
        # Validate input
        ValidationService.validate_chat_message(message)

        # If model explicitly requests mock/free
        if (model and str(model).lower() in ('mock', 'free', 'free-mock')) or use_mock:
            return ChatService._generate_mock_recommendation(message)

        print("[DEBUG] Starting API calls...")

        # Try Groq first (free and fast!)
        groq_key = os.environ.get('GROQ_API_KEY')
        print(f"[DEBUG] Groq key available: {bool(groq_key)}")

        if groq_key:
            try:
                result = ChatService._call_groq(message, groq_key)
                print("[DEBUG] Groq success!")
                return result
            except Exception as e:
                print(f"[DEBUG] Groq failed: {str(e)}")

        # Try OpenAI if API key is available
        api_key = api_key or os.environ.get('OPENAI_API_KEY')

        if api_key:
            try:
                return ChatService._call_openai(message, api_key, model)
            except Exception as e:
                print(f"[DEBUG] OpenAI failed: {str(e)}")

        # Try Hugging Face if token is provided
        hf_token = hf_token or os.environ.get('HUGGINGFACE_API_KEY')

        if hf_token:
            try:
                return ChatService._call_huggingface(message, hf_token, hf_model)
            except Exception as e:
                print(f"[DEBUG] HuggingFace failed: {str(e)}")

        # No API available
        raise BadRequestException(
            "âŒ Nenhuma API de IA disponÃ­vel.\n\n"
            "Para usar IA de verdade, escolha uma opÃ§Ã£o:\n\n"
            "1ï¸âƒ£ Groq (GRÃTIS, rÃ¡pido) â­ RECOMENDADO:\n"
            "   - Crie conta: https://console.groq.com\n"
            "   - Gere API key grÃ¡tis\n"
            "   - Adicione no header 'X-Groq-Key'\n\n"
            "2ï¸âƒ£ OpenAI (pago, ~$0.002/requisiÃ§Ã£o):\n"
            "   - Crie conta: https://platform.openai.com\n"
            "   - Gere API key e adicione no header 'X-OpenAI-Key'\n\n"
            "3ï¸âƒ£ Hugging Face (grÃ¡tis, mais lento):\n"
            "   - Gere token 'Read': https://huggingface.co/settings/tokens\n"
            "   - Adicione no header 'X-HF-Token'\n\n"
            "4ï¸âƒ£ Modo Mock (sem IA):\n"
            "   - Adicione no body: {'model': 'mock'}"
        )

    @staticmethod
    def _call_groq(message: str, api_key: str):
        """Chama API do Groq (GRÃTIS e RÃPIDO)"""
        print(f"[DEBUG] Calling Groq API...")

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": "llama-3.1-8b-instant",
            "messages": [
                {
                    "role": "system",
                    "content": "VocÃª Ã© um bibliotecÃ¡rio experiente. Recomende livros em portuguÃªs, com tÃ­tulo, autor, nÃºmero aproximado de pÃ¡ginas e motivo da recomendaÃ§Ã£o. Seja detalhado e Ãºtil."
                },
                {
                    "role": "user",
                    "content": message
                }
            ],
            "temperature": 0.7,
            "max_tokens": 1000
        }

        response = requests.post(
            ChatService.GROQ_CHAT_COMPLETIONS,
            json=payload,
            headers=headers,
            timeout=30
        )

        print(f"[DEBUG] Groq response status: {response.status_code}")

        if response.status_code == 401:
            raise UnauthorizedException("Groq API key invÃ¡lida. Gere uma nova em https://console.groq.com/keys")

        if response.status_code == 429:
            raise RateLimitException("Rate limit do Groq atingido. Aguarde alguns segundos.")

        if response.status_code != 200:
            raise Exception(f"Groq API error: {response.status_code} - {response.text}")

        data = response.json()
        content = data['choices'][0]['message']['content']

        return f"ğŸ¤– [Groq AI - LLaMA 3]\n\n{content}"

    @staticmethod
    def _call_openai(message: str, api_key: str, model: str = None):
        """Chama API da OpenAI"""
        selected_model = model or os.environ.get('OPENAI_MODEL') or 'gpt-3.5-turbo'

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": selected_model,
            "messages": [
                {
                    "role": "system",
                    "content": "VocÃª Ã© um bibliotecÃ¡rio experiente. Recomende livros em portuguÃªs, com tÃ­tulo, autor, nÃºmero aproximado de pÃ¡ginas e motivo da recomendaÃ§Ã£o."
                },
                {
                    "role": "user",
                    "content": str(message)
                }
            ],
            "max_tokens": 500,
            "temperature": 0.7
        }

        response = requests.post(
            ChatService.OPENAI_CHAT_COMPLETIONS,
            json=payload,
            headers=headers,
            timeout=30
        )

        if response.status_code != 200:
            try:
                err = response.json()
                err_info = err.get('error', {}) if isinstance(err, dict) else {}
                err_type = err_info.get('type')
                err_message = err_info.get('message') or str(err)

                if err_type == 'insufficient_quota' or response.status_code == 402:
                    raise PaymentRequiredException(err_message)
                if response.status_code == 401:
                    raise UnauthorizedException(err_message)
                if response.status_code == 429:
                    raise RateLimitException(err_message)

                raise Exception(f"OpenAI API error: {err}")
            except ValueError:
                raise Exception(f"OpenAI API returned status {response.status_code}.")

        data = response.json()
        try:
            return data['choices'][0]['message']['content']
        except Exception:
            raise Exception("Resposta inesperada da OpenAI API.")

    @staticmethod
    def _call_huggingface(message: str, token: str, model: str = None):
        """Chama API do Hugging Face"""
        selected_model = model or 'gpt2'
        url = f"https://api-inference.huggingface.co/models/{selected_model}"

        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }

        prompt = f"Recomende um livro sobre: {message}\nRecomendaÃ§Ã£o:"

        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": 200,
                "temperature": 0.7,
                "return_full_text": False
            },
            "options": {
                "wait_for_model": True
            }
        }

        response = requests.post(url, json=payload, headers=headers, timeout=60)

        if response.status_code == 401:
            raise UnauthorizedException(
                "Token do Hugging Face invÃ¡lido. Gere um novo token com permissÃ£o 'Read' em https://huggingface.co/settings/tokens")

        if response.status_code == 429:
            raise RateLimitException("Rate limit atingido no Hugging Face.")

        if response.status_code != 200:
            raise Exception(f"Hugging Face API error: {response.status_code} - {response.text}")

        try:
            data = response.json()
            if isinstance(data, list) and len(data) > 0:
                return data[0].get('generated_text', str(data))
            return str(data)
        except:
            return response.text

    @staticmethod
    def _generate_mock_recommendation(message: str) -> str:
        """Gera resposta inteligente baseada em palavras-chave"""
        msg_lower = message.lower()

        # Verifica se Ã© uma pergunta sobre livros/recomendaÃ§Ã£o
        is_book_request = any(word in msg_lower for word in [
            'livro', 'book', 'ler', 'leitura', 'recomend', 'sugest', 'indic'
        ])

        # Se NÃƒO for sobre livros, responde de forma genÃ©rica
        if not is_book_request:
            if any(word in msg_lower for word in ['olÃ¡', 'oi', 'hello', 'hey']):
                return "OlÃ¡! ğŸ‘‹\n\nSou um assistente de recomendaÃ§Ãµes de livros. Como posso ajudar?"

            return f"Recebi sua mensagem, mas estou operando em modo limitado.\n\nğŸ’¡ Para ter uma IA completa, adicione a chave do Groq (grÃ¡tis) no header 'X-Groq-Key'.\n\nPosso ajudar com recomendaÃ§Ãµes de livros se preferir!"

        # Se for sobre livros, continua com o sistema de recomendaÃ§Ãµes
        recommendations = []

        if any(word in msg_lower for word in ['ia', 'inteligÃªncia artificial', 'ai', 'machine learning']):
            recommendations = [
                "ğŸ“š 'InteligÃªncia Artificial' de Stuart Russell e Peter Norvig â€” ~1200 pÃ¡ginas",
                "ğŸ“š 'Vida 3.0' de Max Tegmark â€” ~380 pÃ¡ginas",
                "ğŸ“š 'SuperinteligÃªncia' de Nick Bostrom â€” ~350 pÃ¡ginas"
            ]
        elif any(word in msg_lower for word in ['python', 'programaÃ§Ã£o']):
            recommendations = [
                "ğŸ“š 'Python Fluente' de Luciano Ramalho â€” ~800 pÃ¡ginas",
                "ğŸ“š 'Automatize Tarefas MaÃ§antes' de Al Sweigart â€” ~500 pÃ¡ginas"
            ]
        else:
            recommendations = [
                "ğŸ“š 'Sapiens' de Yuval Harari â€” ~450 pÃ¡ginas",
                "ğŸ“š 'O Gene EgoÃ­sta' de Richard Dawkins â€” ~360 pÃ¡ginas"
            ]

        result = "[MODO MOCK]\n\n"
        result += "\n".join(recommendations)
        result += "\n\nğŸ’¡ Para IA real grÃ¡tis, use Groq: https://console.groq.com"

        return result